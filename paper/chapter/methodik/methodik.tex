\chapter{Methodik}
Hier keine Details zur Implementierung!
\section{Vorverarbeitung}
\subsection{Punktwolken}
\begin{enumerate}
  \item Beschreibung der Daten ?!
  \item Normalisierung
  \item Randomisiertes erweitern
  \item PCA
\end{enumerate}
\input{cloud_norm.pgf}
\subsection{Feature Extrahieren}
\subsubsection{Statische Features}
Dazu schreiben, wieso wir finden, dass das Feature ein gutes ist (z.B. weil es den Feature-Raum verkleinert, ...)
\begin{enumerate}
  \item XYFeature
  \item Orientation
  \item EuclidianDistance
  \item CenterDistance
  \item CenterOrienation
  \item Interpolation
\end{enumerate}
\input{interpolation_extraction.pgf}
\subsubsection{Zeitliche Features}
\begin{itemize}
\item TimeDifferential
\end{itemize}
\input{time_differential.pgf}
\subsubsection{Featureverarbeitung}
Zweck mitbeschreiben (z.B. PCA -> FeatureRaum weiter reduzieren)
\begin{itemize}
  \item Negativanteil verringern
  \item MinMax/MeanVar Normalisieren
  \item Shufflen
  \item PCA
\end{itemize}
\subsubsection{Klassifikatoren}
SVM + Random Forests, Art von Parametern

Vielleicht noch eine Beschreibung einer allgemeinen Pipeline.
\section{Evaluierungsmethoden}
Die im vorherigen Abschnitt beschrieben Methoden zur Feature Extraction,
Verarbeitung und Klassifikation sollen in verschiedenen Kombinationen evaluiert
werden. Der erste Datensatz aus 10 Personen wird dazu aufgeteilt in 60\%
Trainingsmenge und 40\% Validierungsmenge. Hier ist die Entscheidung zu treffen,
wie die Personen auf die Mengen aufgeteilt werden:
\begin{enumerate}
\item Erst die Frames durchmischen, dann aufteilen: Dies ist sinnvoll, wenn der
  Klassifikator nur verwendet werden soll, um Action Units in neuen Frames von schon bekannten
  Personen zu erkennen. Es wird nicht getestet, wie gut der Klassifikator auf
  neue Personen generalisiert!
  \item 6 Personen nur im Training, 4 nur in der Validierung verwenden: Die
    Performance auf der Validierungsmenge ist repräsentativ dafür, wie gut der
    Klassifikator Action Units bei bisher unbekannten Personen erkennt
\end{enumerate}
Erste Tests haben gezeigt, dass Methode 1 zu deutlich besserern
Performancestatistiken führt. Wir haben uns aber für Methode 2 entschieden, weil
die Generalisierung auf neue Personen das interessantere Problem ist: In
Anwendungsfällen ist es wünschenswert, für neue Personen nicht erst mehrere
tausend Frames manuell labeln zu müssen, um den Klassifikator auf dieser Person
zu trainieren.

Aufgrund der geringen Anzahl positiver Samples (Frames, in denen die Action Unit
aktiviert ist), ist die Accuracy keine zuverlässige Statistik. Ein Klassifikator,
der die Action Unit immer als ``nicht aktiv'' klassifiziert, könnte sehr hohe
Accuracy erreicht, ohne tatsächlich etwas über die Action Unit gelernt zu haben.
Stattdessen evaluieren wir die Klassifikatoren anhand von 
% \begin{table}[h]
%   \def\arraystretch{2.5}
%   \centering
%   \begin{tabular}{l c}
% Precision: & $\displaystyle\frac{TP}{TP+FP}$ \\
% Recall: & $\displaystyle\frac{TP}{TP+FN}$ \\
% F1 score: & $\displaystyle 2\frac{\text{Precision}*\text{Recall}}{\text{Precision}+\text{Recall}}$
%   \end{tabular}
% \end{table}
\begin{equation*}
\text{Precision}=\frac{TP}{TP+FP}\,, \qquad \text{Recall}=\frac{TP}{TP+FN}\,, \qquad \text{F1 score}=2\cdot\frac{\text{Precision}\cdot\text{Recall}}{\text{Precision}+\text{Recall}}\,.
\end{equation*}

Der F1 score ist das harmonische Mittel zwischen Precision und Recall. Da wir
von der Aufgabenstellung her keine Präferenz für hohe Precision / hohen Recall
haben, nutzen wir den F1 score als erste Zahl zum Vergleich der Klassifikatoren.

Es werden alle Kombinationen aus Feature Extraction, Klassifikator und Parametern
auf dem ersten Datensatz trainiert und evaluiert. Anschließend werden die besten
fünf pro Action Unit anhand des F1 scores ausgewählt. Da pro Action Unit ca. 160
Kombinationen evaluiert werden, kann es durch diese Auswahl der besten fünf zu
einem Overfitting gegen die Validierungsmenge kommen. Um realistische Zahlen für
die Performance zu bekommen, werden deshalb die besten fünf nochmal auf einer
Testmenge evaluiert. Diese besteht aus fünf bisher unbekannten Personen aus
einem zweiten Datensatz.

\begin{itemize}
\item Welche Parameter/Pipeline zum trainieren
  \begin{itemize}
  \item Warum diese Parameter und keine anderen?
  \end{itemize}
\end{itemize}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../paper"
%%% End: 