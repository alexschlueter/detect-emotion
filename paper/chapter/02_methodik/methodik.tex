\chapter{Methodik}\label{ch:methodik}
In diesem Kapitel werden sowohl die von uns verwendeten Methoden zum Training der Klassifikatoren und zum Klassifizieren,
als auch die von uns verwendeten Klassifikatoren selbst genauer beschrieben. Außerdem wird erläutert, wie die Ergebnisse
evaluiert wurden.

\section{Ground-Truth Daten}
Wie zuvor erwähnt, besitzt eine Action Unit eine Ausprägung $I$ zwischen $0$ und $5$. Um die spätere Klassifikation zu vereinfachen wollen wir das Problem vereinfachen, indem wir die in zwei Kategorien klassifizieren: Action und keine Action. Dazu setzen wir ein Schwellenwert $0<h\leq5$ ein. Falls $I<h$, so gehen wir von keiner Action aus. Falls $h\leq I$, so wollen wir davon ausgehen, dass es eine Action gegeben hat.
\section{Vorverarbeitung}
\subsection{Aufbereitung der Eingabedaten}
Wie bereits in der Einleitung erwähnt, handelt es sich bei den Eingabedaten um 12 Videos, die der DISFA Datenbank entnommen sind.
Die Videos wurden nacheinander aufgenommen und zeigen verschiedene Probanden. Bedingt dadurch, sind die Landmarks
in den Videos nicht identisch bezüglich Skalierung, Rotation und Position.\newline
Damit die Klassifikation durch diese Störungen nicht beeinträchtigt wird, werden die Eingabedaten zunächst normalisiert. Dies geschieht
in drei Schritten.
\begin{enumerate}
  \item Die Landmarks werden um den Koordinaten-Ursprung zentriert. Hierzu berechnen wir einen Vektor vom Mittelpunkt aller Landmarks zum Ursprung und translatieren
        die gesamte Punktwolke um diesen Vektor.
  \item Daraufhin wird die Punktwolke so skaliert, dass die maximale horizontale Distanz alle Landmarks genau 1 beträgt. Hierzu berechen wir diese maximale Distanz und
        teilen alle Koordinaten der Landmarks durch diese.
  \item Um Störungen durch Drehung des Kopfes der Probanden auszugleichen, normalisieren wir ebenfalls die Rotation der Punktwolke. Dazu berechnen wir einen Vektor
        zwischen den beiden Augen des Probanden und rotieren die gesamte Punktwolke so, dass dieser Vektor auf eine Rotation von 0° gebracht wird \ref{Methodik.CloudNormalization}
\end{enumerate}
\input{cloud_norm.pgf}\label{Methodik.CloudNormalization}

Ein weiteres Problem der Eingabedaten besteht darin, dass die Relation von true-positives zu true-negatives sehr gering ist, das heisst die Anzahl der Frames
in denen eine bestimmte Action Unit aktiv ist, ist für die meisten Action Units relativ gesehen sehr gering.\newline
Um dieses Problem zu reduzieren, ermöglichen wir es die true-positives der Eingabedaten zu erweitern, indem die entsprechenden Frames dupliziert
und die Landmarks in diesen Frames durch eine leichte, normalverteilte Störung verschoben werden.


\subsection{Feature Extraction}
Bei der (visuellen) Emotionserkennung wird versucht anhand von einem oder mehreren, verschiedenen Merkmalen (engl. features)
einem Gesicht eine oder mehrere Emotionen zuzuordnen. Je mehr Aussagekraft die Kombination dieser Merkmale über die jeweiligen Emotionen haben,
desto besser können diese klassifiziert werden. Das Problem dabei ist, dass meist weder die Merkmale, noch
ihre Aussagekraft zuvor bekannt sind. Deshalb extrahieren wir aus den Eingabedaten, also den Videos mit je 66 Landmarks pro
Frame, verschiedene Merkmale, um sie in verschiedenen Kombinationen miteinander zu testen. Es folgt eine Beschreibung
der von uns verwendeten Features.

\subsubsection{Statische Features}
Mit statischen Features bezeichnen wir solche, die anhand eines Gesichtes zu genau einen Zeitpunkt berechnet werden. 
\begin{enumerate}
  \item \textbf{X-/Y-Koordinaten}: die Koordinaten der Landmarks werden als Merkmale genutzt. Da in der Menge der Koordinaten sowohl Informationen
        über die individuellen Punkte liegen, als auch Informationen über ihre Relation zueinander, ist es sinnvoll dieses Feature zu testen.

  \item \textbf{Paarweise Orientierung}: es werden jeweils alle Paare von je zwei unterschiedlichen Landmarks betrachtet und die Rotation des Vektors zwischen
        den beiden Punkten als Merkmal genutzt. Weil sich bei verschiedenen Mimiken meist die Position markanter Punkte im Gesicht zueinander ändert, erscheint es sinnvoll
        Features zu nutzen, die die Landmarks untereinander explizit in Relation setzen.

  \item \textbf{Paarweise Euklidische Distanz}: auch hier werden jeweils alle Paare unterschiedlicher Landmarks betrachtet und die euklidische Distanz zwischen
        den beiden Punkten als Merkmal genutzt. Dieses Feature erscheint ebenfalls sinnvoll, weil es Informationen über die Relation von Landmarks untereinander hat.

  \item \textbf{Orientierung relativ zum Mittelpunkt der Landmarks}: bei diesem Feature wird die Orientierung jedes Landmarks relativ zum Mittelpunkt aller Landmarks betrachtet,
        das heisst es wird die Rotation des Vektors zwischen Mittelpunkt und Landmarks als Merkmal genutzt. Dieses Feature enthält Informationen darüber, wie die Position der Landmarks relativ
        zum gesamten Gesicht ist. Dies erscheint für viele Gesichtsausdrücke sinnvoll.

  \item \textbf{Euklidische Distanz zum Mittelpunkt der Landmarks}: dieses Feature betrachtet die euklidische Distanz jedes Landmarks zum Mittelpunkt aller Landmarks. Dieses Feature sagt ebenfalls
        etwas über die Relation der einzelnen Landmarks zum gesamten Gesicht aus.

  \item \textbf{Polynominterpolation}: 
     hierbei werden verschiedene Bereiche der Gesichts einzeln betrachtet (z.B. die Oberlippe, siehe Abbildung \ref{fig:interpol}).
     Diese Landmarks, das heißt die Punkte, werden zuerst normiert. Dazu verschiebt man die dortigen Punkte, so dass der Mittelpunkt der Koordinaten-Ursprung ist. Danach werden sie so skaliert, dass die maximale Entfernung in $x$-Richtung gleich $1$ ist.
     Nach dieser Normierung werden diese Punkte durch ein Polynom interpoliert. Hierzu wird die Methode der kleinsten Quadrate benutzt. Die dadurch berechnete Koeffizienten des Polynoms werden nun verwendet. Sie ergeben nämlich mit den Polynomkoeffizienten der anderen Gesichtsbereiche die Features.\\
     Dieses Verfahren hat folgende Vorteile: 
     %Erstens entsprechen die Punkte der einzelnen Gesichtsbereiche meist die wichtigen Punkte einer Action Unit.
     Erstens fließt ein eventuelles Rauschen durch Interpolation anhand mehrerer Punkte  weniger in die resultierende Feature mit ein. Und zweitens wird der Feature-Raum verringert, da mehrere Punkte durch wenige Koeffizienten repräsentiert werden.
\end{enumerate}
\begin{figure}
\scalebox{ 0.9 }{
\input{interpolation_extraction.pgf}
}
\caption{InterpolationFeatureExtraction}
\label{fig:interpol}
\end{figure}

\subsubsection{Zeitliche Features}
Bisher haben wir den zeitlichen Aspekt bei der Berechnung der Features außer Acht gelassen. Die oben aufgelisteten Features werden anhand einer der Landmarks zu einen gewissen Zeitpunkt berechnet.
Man kann jedoch die oben aufgelisteten Arten von Featureberechnung um einen zeitlichen Aspekt erweitern, indem wir ihre Veränderung über die Zeit als Feature benutzen. 
Um die zeitliche Ableitung zu einen bestimmten Zeitpunkt zu berechnen, wird
numerisch Differentation benutzt. Es bezeichne $t\in\mathbb{N}$ den aktuellen Zeitpunkt und $f(t)\in\mathbb{R}^d,\;d\in\mathbb{N}$ den Feature-Vektor einer der obigen Feature-Extraktoren. Nun ergibt sich das zeitliche Feature durch die numerische Ableitung $\frac{f(t+\delta)-f(t-\delta)}{2}$. Üblicherweise nimmt man $\delta=1$ an, doch da sich eventuell ein aussagekräftigeres Resultat ergibt, wenn man eine größeren zeitliche Abstand benutzt, wollen wir hier $\delta\in\mathbb{N}\setminus\{0\}$  beliebig voraussetzen.

Als Beispiel nehmen wir die zeitliche Ableitung der XY-Koordinaten (siehe dazu Abbildung \ref{fig:timediff}). 
Hier werden zum Zeitpunkt $t+1$ und $t-1$ die Koordinaten voneinander subtrahiert und halbiert. Zum Beispiel $\Delta v_{43} \coloneqq \frac{v_{43}(t+1) - v_{43}(t-1)}{2}\in\mathbb{R}^2$ (wobei $v_{j}(t)$ die Koordinate des Punktes $j\in\{0,\ldots,65\}$ zum Zeitpunkt $t$ ist).
Diese sich so ergebene Differenz dient nun als neues Feature.

\begin{figure}
\input{time_differential.pgf}
\caption{TimeDifferentialExtraction}
\label{fig:timediff}
\end{figure}

Nun können jedoch nicht mehr die Intensität der Action Units als Vorlage für eine Action oder keine Action benutzt werden. 
Um das zu veranschaulichen gehen wir von einen Zeitraum aus, an den sich eine Action ereignet, aber ihre Intensität nicht ändert.
Da keine Änderung passiert, wird sich auch in den errechneten Features kaum was ändern. Die zeitliche Ableitung wird also nahe $0$ sein, soll aber als Action klassifiziert werden. Die Ableitung ist aber auch nahe $0$, wenn sich kaum was ändert und keine Action ereignet.
Um dieses Problem zu lösen, betrachtet man die ebenfalls die zeitliche Differenz der Intensität der Action Unit. Wir subtrahieren hierfür die Intensität zum Zeitpunkt $t+1$ und $t-1$. Falls die absolute zeitliche Differenz einen Schwellenwert überschreitet, gehen wir von einer Action aus, sonst von keiner. Somit ist sowohl der Beginn als auch das Ende einer Action Unit eine Action.

\subsubsection{Featureverarbeitung}
Zweck mitbeschreiben (z.B. PCA -> FeatureRaum weiter reduzieren)
\begin{itemize}
  \item Negativanteil verringern
  \item MinMax/MeanVar Normalisieren
  \item Shufflen
  \item PCA
\end{itemize}

\subsubsection{Klassifikatoren}
SVM + Random Forests, Art von Parametern
Vielleicht noch eine Beschreibung einer allgemeinen Pipeline.


\section{Evaluierungsmethoden}
Die im vorherigen Abschnitt beschrieben Methoden zur Feature Extraction,
Verarbeitung und Klassifikation sollen in verschiedenen Kombinationen evaluiert
werden. Der erste Datensatz aus 10 Personen wird dazu aufgeteilt in 60\%
Trainingsmenge und 40\% Validierungsmenge. Hier ist die Entscheidung zu treffen,
wie die Personen auf die Mengen aufgeteilt werden:
\begin{enumerate}
\item Erst die Frames durchmischen, dann aufteilen: Dies ist sinnvoll, wenn der
  Klassifikator nur verwendet werden soll, um Action Units in neuen Frames von schon bekannten
  Personen zu erkennen. Es wird nicht getestet, wie gut der Klassifikator auf
  neue Personen generalisiert!
  \item 6 Personen nur im Training, 4 nur in der Validierung verwenden: Die
    Performance auf der Validierungsmenge ist repräsentativ dafür, wie gut der
    Klassifikator Action Units bei bisher unbekannten Personen erkennt
\end{enumerate}
Erste Tests haben gezeigt, dass Methode 1 zu deutlich besserern
Performancestatistiken führt. Wir haben uns aber für Methode 2 entschieden, weil
die Generalisierung auf neue Personen das interessantere Problem ist: In
Anwendungsfällen ist es wünschenswert, für neue Personen nicht erst mehrere
tausend Frames manuell labeln zu müssen, um den Klassifikator auf dieser Person
zu trainieren.

Aufgrund der geringen Anzahl positiver Samples (Frames, in denen die Action Unit
aktiviert ist), ist die Accuracy keine zuverlässige Statistik. Ein Klassifikator,
der die Action Unit immer als ``nicht aktiv'' klassifiziert, könnte sehr hohe
Accuracy erreichen, ohne tatsächlich etwas über die Action Unit gelernt zu haben.
Stattdessen evaluieren wir die Klassifikatoren anhand von 
% \begin{table}[h]
%   \def\arraystretch{2.5}
%   \centering
%   \begin{tabular}{l c}
% Precision: & $\displaystyle\frac{TP}{TP+FP}$ \\
% Recall: & $\displaystyle\frac{TP}{TP+FN}$ \\
% F1 score: & $\displaystyle 2\frac{\text{Precision}*\text{Recall}}{\text{Precision}+\text{Recall}}$
%   \end{tabular}
% \end{table}
\begin{equation*}
\text{Precision}=\frac{TP}{TP+FP}\,, \qquad \text{Recall}=\frac{TP}{TP+FN}\,, \qquad \text{F1 score}=2\cdot\frac{\text{Precision}\cdot\text{Recall}}{\text{Precision}+\text{Recall}}\,.
\end{equation*}

Der F1 score ist das harmonische Mittel zwischen Precision und Recall. Da wir
von der Aufgabenstellung her keine Präferenz für hohe Precision / hohen Recall
haben, nutzen wir den F1 score als erste Zahl zum Vergleich der Klassifikatoren.

Es werden alle Kombinationen aus Feature Extraction, Klassifikator und Parametern
auf dem ersten Datensatz trainiert und evaluiert. Anschließend werden die besten
fünf pro Action Unit anhand des F1 scores ausgewählt. Da pro Action Unit ca. 160
Kombinationen evaluiert werden, kann es durch diese Auswahl der besten fünf zu
einem Overfitting gegen die Validierungsmenge kommen. Um realistische Zahlen für
die Performance zu bekommen, werden deshalb die besten fünf nochmal auf einer
Testmenge evaluiert. Diese besteht aus fünf bisher unbekannten Personen aus
einem zweiten Datensatz.

% \begin{itemize}
% \item Welche Parameter/Pipeline zum trainieren
%   \begin{itemize}
%   \item Warum diese Parameter und keine anderen?
%   \end{itemize}
% \end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../paper"
%%% End:
